---
title: 'RAPHAEL:Text-to-Image Generation via Large Mixture of Diffusion Paths'
date: 2024-05-16
permalink: /posts/2024/05/raphael
tags:
  - raphael
  - diffusion model
  - Mixture of Experts
---

In recent years Generative Artificial Intelligence has gained a lot of popularity and usage in our daily lives. Image Generation became popular with models like Stable Diffusion or DALL-E which showed the public what Image Generation is able to do. In this blog post, I aim to introduce and explain a new model, RAPHAEL, which outperforms models like Stable Diffusion and focuses on accurately displaying text in the generated images.

Disclaimer: This is a draft of the final blog post, so further details will be added in most sections

Why image generation?
======
Image Generation can be used to innovate in many areas. For example it can be used to vastly improve the speed and reduce the cost of making marketing ads, for example to create a simple poster we can just use the model to generate it for us instead of hiring actors, buying equipement and so on. Another area would be film making, where instead of manually animating scenes, which is time consuming and expensive, we can just generate the scenes. There are also many applications in Science and Engineering like for example discovering new drugs where these models can be used to find new molecules, which else would be a very time and money consuming process.

Recent models like Stable Diffusion or DALL-E have shown great success but often lack the ability to accurately display text in the generated images like shown below.

<img width="668" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/6431ef1f-57b2-4473-a938-a13dc402b5de">

RAPHAEL aims at accurately displaying text in generated images and overall improving image quality.

How to map image generation to a Deep Learning problem
======

Input 
------
The Input for Text-Conditional Image Diffusion Models is just plain text in the application phase. 

Between the user and the Diffusion Model sits an encoder neural network which is responsible for extracting text tokens from the text and embedding them.
The embedding of a text token is a very large vector with many values. Like shown below the embedding vectors transport semantics, for example the difference between the embedding of man and woman is similiar to the difference of uncle and aunt. 

<img width="442" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/91b74c71-9a42-4c27-8be5-f26389a05fda">

While training there is also a second input source for learning the denoising of images by feeding the model images, but I won't go into detail about that here.

Output
------
The output of the model are image tokens which are decoded to images by an image decoder.


Loss function 
------

The model uses two loss function combined, by addding them: $L = L_{denoise} + L_{edge}$

The first loss function $L_{denoise} = E_{t,x_0,ϵ∼N(0,I)} ∥ϵ − D_θ (x_t, t)∥^2_2$ is used for the denoising neural network, while the loss function $L_{edge} = Focal(P_θ(M),I_{edge})$ is used to train the transformer blocks with Edge-supervised Learning, Focal(·, ·) denotes the focal loss [19] employed to measure the discrepancy between the predicted and the “ground-truth” edge maps.

Further settings for training 
------
The model uses an AdamW optimizer with a learning rate of 1e-04 for training.

AdamW optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments with an added method to decay weights per the techniques discussed in the paper, 'Decoupled Weight Decay Regularization' by Loshchilov, Hutter et al., 2019. (https://keras.io/api/optimizers/adamw/)

The model uses weight decay as regularization and a batch size of 2,000 for training. 

RAPHAEL Architecture
======

Comparison to Stable Diffusion
------

What are Mixture of Experts?
------
In general Mixture of Experts (MoE) is the method of using multiple expert models for dividing a problem instead of using just a single big model.

In Transformer blocks the MoE design is normally implemented by replacing the MLP after the Attention Layer with a Text Gate Network folloed by the multiple expert MLPs.

In the RAPHAEL model a transformer block consists of four key components, the Self Attention Layer, the Cross Attention Layer, the Time-MoE Layer and the Space-MoE Layer as the following image shows:

<img width="414" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/e5d19f96-5af1-4ba7-97d7-80fc75212dfc">

What are Time-MoE?
------
Time-MoE is a MoE Layer which assigns the image in different denoising time steps to different expert models.

A Time-MoE layer constists of a Text Gate Network and the expert models as shown in the image below. It takes the time step and the features as input.

<img width="225" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/d3d0eb5b-1656-4932-83ca-f3c4638861f0">

The Time Gate Network does the assignment using the following formula: $t\_router(t_i)=argmax(softmax(G′(E′_θ(t_i))+ϵ))$

The result of the assignments can be seen in the image below:

<img width="597" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/37ea0970-9829-4005-9946-cb757f6d62ba">

What are Space-MoE?
------
Space-MoE is a MoE Layer which assigns specific text tokens to image regions.

A Space-MoE layer constists of a Text Gate Network and the expert models as shown in the image below. It takes text as input.

<img width="335" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/3cefc66c-c482-49a0-a75b-f0bbafaba431">

The Text Gate Network does the assignment using the following formula: $route(y_i)=argmax(softmax(G(E_θ(y_i))+ϵ))$

The output of the Space-MoE Layer is built by taking the mean of all expert models, calculated by the following formula: 
$\frac{1}{n_y} \sum_{i=1}^{n_y} e_{route(y_i)}(h′(x_t) \cdot M_i)$

As a result of the Space-MoE Layer, as shown in the picture below, different categories activate different diffusion paths 

<img width="316" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/681216f0-19cd-4359-abef-51d55217c280">


What is Edge-supervised Learning
------

Experiments and Benchmarks
======

Discussion
======
