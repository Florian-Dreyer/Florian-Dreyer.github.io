---
title: 'RAPHAEL:Text-to-Image Generation via Large Mixture of Diffusion Paths'
date: 2024-05-16
permalink: /posts/2024/05/raphael
tags:
  - raphael
  - diffusion model
  - Mixture of Experts
---

In recent years Generative Artificial Intelligence has gained a lot of popularity and usage in our daily lives. Image Generation became popular with models like Stable Diffusion or DALL-E which showed the public what Image Generation is able to do. In this blog post, I aim to introduce and explain a new model, RAPHAEL, which outperforms models like Stable Diffusion and focuses on accurately displaying text in the generated images.

Why image generation?
======
Image Generation can be used to innovate in many areas. For example it can be used to vastly improve the speed and reduce the cost of making marketing ads, for example to create a simple poster we can just use the model to generate it for us instead of hiring actors, buying equipement and so on. Another area would be film making, where instead of manually animating scenes, which is time consuming and expensive, we can just generate the scenes. There are also many applications in Science and Engineering like for example discovering new drugs where these models can be used to find new molecules, which else would be a very time and money consuming process.

Recent models like Stable Diffusion or DALL-E have shown great success but often lack the ability to accurately display text in the generated images like shown below.

<img width="668" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/6431ef1f-57b2-4473-a938-a13dc402b5de">

RAPHAEL aims at accurately displaying text in generated images and overall improving image quality.

How to map image generation to a Deep Learning problem
======

Input 
------
The Input for Text-Conditional Image Diffusion Models is just plain text in the application phase. 

Between the user and the Diffusion Model sits an encoder neural network which is responsible for extracting text tokens from the text and embedding them.
The embedding of a text token is a very large vector with many values. Like shown below the embedding vectors transport semantics, for example the difference between the embedding of man and woman is similiar to the difference of uncle and aunt. 

<img width="442" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/91b74c71-9a42-4c27-8be5-f26389a05fda">

While training there is also a second input source for learning the denoising of images by feeding the model images, but I won't go into detail about that here.

Output
------
The output of the model are image tokens which are decoded to images by an image decoder.


Loss function 
------

The model uses two loss function combined, by addding them: $L = L_{denoise} + L_{edge}$

The first loss function $L_{denoise} = E_{t,x_0,ϵ∼N(0,I)} ∥ϵ − D_θ (x_t, t)∥^2_2$ is used for the denoising neural network, while the loss function $L_{edge} = Focal(P_θ(M),I_{edge})$ is used to train the transformer blocks with Edge-supervised Learning, Focal(·, ·) denotes the focal loss [19] employed to measure the discrepancy between the predicted and the “ground-truth” edge maps.

Further settings for training 
------
The model uses an AdamW optimizer with a learning rate of 1e-04 for training.

AdamW optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments with an added method to decay weights per the techniques discussed in the paper, 'Decoupled Weight Decay Regularization' by Loshchilov, Hutter et al., 2019. (https://keras.io/api/optimizers/adamw/)

The model uses weight decay as regularization and a batch size of 2,000 for training. 

RAPHAEL Architecture
======

Comparison to Stable Diffusion
------

What are Mixture of Experts?
------

What are Space-MoE?
------

What are Time-MoE?
------

What is Edge-supervised Learning
------

Experiments and Benchmarks
======

Discussion
======
