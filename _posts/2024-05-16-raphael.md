---
title: 'RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths'
date: 2024-05-31
permalink: /posts/2024/05/raphael
tags:
  - RAPHAEL
  - Diffusion Model
  - Mixture of Experts
  - Computer Vision
  - Generative Model
---

In recent years Generative Artificial Intelligence has gained a lot of popularity and usage in our daily lives. Image Generation became popular with models like Stable Diffusion or DALL-E which showed the public what Image Generation is able to do. \
In this blog post, I aim to introduce and explain a new model, RAPHAEL, which outperforms models like Stable Diffusion and focuses on accurately displaying text in the generated images [[1]](#1).

Disclaimer: This is a draft of the final blog post, so further details will be added in future versions

Why image generation?
======
Have you ever taken a photo and were disappointed with the result because it did not have the level of detail you hoped for? With image generation that problem can be solved, because the models which are used to generate entirely new images can also be used to sharpen existing ones. \
As great as this already seems for daily usage, it brings even more advantages in the usage for science. For example in Biology, scientists use Diffusion Models to improve the image quality of their microscopes, which helps them to gain new insights and also accelerate their previous work. 

A more radical application of Image Generation is in the field of chemistry where it is used to generate new molecules for a specific purpose, which vastly accelerates the process of finding the right molecule for a given problem, for example in drug discovery. 

Recent models like Stable Diffusion or DALL-E have shown great success but often lack the ability to accurately display text in the generated images like shown below:

<img width="668" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/6431ef1f-57b2-4473-a938-a13dc402b5de">

RAPHAEL aims at accurately displaying text in generated images and overall improving image quality.

What is a Diffusion Model?
======
Diffusion Models have the goal to generate images by removing noise from a pure noise.  
![image](https://github.com/Florian-de/Florian-de.github.io/assets/64322175/ca2a8f9b-b597-473c-af9d-9263b7f4cb41)

Image from Medium [[2]](#2)

As shown in the picture above, Diffusion Models constist of two parts, the forward diffusion process and the reverse diffusion process. 

In the forward diffusion process the model takes an image as input and adds random noise to the image step by step starting with the input image $x_0$ and ending in pure noise $x_t$. \
In each step the process is defined as $q(x_t|x_{t−1}) := \mathcal{N}(x_t; \sqrt{1 − \beta_t}x_{t−1},\beta_tI)$ where $q$ is the process, $x_t$ the output of the current step, $x_{t-1}$ the output of the previous step and $\mathcal{N}$ the normal distribution with $\sqrt{1 − β_t}x_{t−1}$ as the mean $\mu$ and $\beta_tI$ as the variance $\sigma^2$. \
During this process $\beta_t$ is controlled by a schedule and has values in the range of 0 and 1. Such a schedule could be as simple as a linear schedule which would increase $\beta_t$ by a constant size each step, but in practice more advanced schedules are used. \
For efficient computation the entire process from $x_0$ to $x_t$ can be calculated using a closed form $q(x_t|x_0) = \mathcal{N}(x_t;\sqrt{\bar\alpha_t}x_0, (1 − \bar\alpha_t)I)$ where $\alpha_t := 1 − β_t$ and $\bar\alpha_t := \Pi^{t}_{s=1} \alpha_s$.

In the backward diffusion process the model tries to predict the total noise for each timestep starting with the pure noise $x_t$ and ending in a denoised image $x_0$. \
The process for each step can be described by the formula $x_{t-1} = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{\beta_t}{\sqrt{1-\bar\alpha}}\epsilon_\theta(x_t, t)) + \sqrt{\beta_t}\epsilon$ where $\epsilon_\theta(x_t, t)$ is output of the prediction model. \
For the prediction of the noise models typically use a modified UNet Neural Network Architecture. 
<img width="920" alt="image" src="https://github.com/Florian-de/Florian-de.github.io/assets/64322175/145c6a61-f5cf-447d-b5d1-e35c4f31b526">

Image from Kemal Erdem [[3]](#3)

An example for such an architecture for a text-conditional model is shown above. \
It takes the total noise an text as input. \
The text is embedded by an encoder network which takes the text as input and has vectors as output. Each vector represents a single text token. Like shown below the embedding vectors transport semantics, for example the difference between the embedding of man and woman is similiar to the difference of uncle and aunt. 

<img width="442" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/91b74c71-9a42-4c27-8be5-f26389a05fda">

Image from 3Blue1Brown [[4]](#4)

The network starts and ends with a pink rectengle which represents a ResNet block which takes the data from the previous layer as input. It is used to extract features from the image.\
The blue rectangles represent Downsample Blocks which takes data from the previous layer and data about the timestamp and the text embeddings as the two inputs. It is used to downsample the data from the previous layer to the size of the layer. \
The grey arrows represent skipping connections between the Downsampling and the Upsampling Blocks to prevent loss of information. \
The green rectangles represent Upsample Blocks which takes data from the previous layer, data about the timestamp and the text embeddings and data from the skipping connection as the three inputs. It is used to predict the noise. \
The orange rectangles represent Self-Attention Blocks which takes the data from the previous Downsample/Upsample Block as input. It is used to learn the connections in the different parts of the image. 

What are Mixture of Experts
======
I will add this section

How to map image generation to a Deep Learning problem
======

Input 
------
The input for RAPHAEL consists of images, complete noise and text. \
The images are used in the forward diffusion process as explained in the section for Diffusion Models. \
In the "application phase" we only need the complete noise and text. 

Output
------
The output of the model are image tokens which are decoded to images by an image decoder.


Loss function 
------

The model uses two loss function combined, by addding them: \$L = L_{denoise} + L_{edge}\$

The first loss function \$L_{denoise}=E_{t,x_0,ϵ∼N(0,I)}∥ϵ−D_θ(x_t,t)∥^2_2\$ is used for the denoising neural network, while the loss function \$L_{edge}=Focal(P_θ(M),I_{edge})\$ is used to train the transformer blocks with Edge-supervised Learning, Focal(·, ·) denotes the focal loss [19] employed to measure the discrepancy between the predicted and the “ground-truth” edge maps.

Further settings for training 
------
The model uses an AdamW optimizer with a learning rate of 1e-04, weight decay as regularization and a batch size of 2,000 for training. 

AdamW optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments with an added method to decay weights per the techniques discussed in the paper, 'Decoupled Weight Decay Regularization' by Loshchilov, Hutter et al., 2019. (https://keras.io/api/optimizers/adamw/)

RAPHAEL Architecture
======

In general a Diffusion Model consists of two parts: the forward process and the denoising network.

In the forward process the image will get more and more random during the steps of adding gaussian noise with the following formula: \$x_t = \sqrt{1-\bar{\alpha}_t}x_0 + \sqrt{\bar{\alpha}_t}ϵ_t\$, where \$\bar{\alpha}_t = \Pi _{i=1} ^{t} \alpha_i\$, \$x_0\$ as the original image and \$\epsilon_t\$ being normally distributed as the noise. 

RAPHAEL uses an U-Net Architecture for the denoising network. It consists of 16 transformer blocks, which employ Mixture of Experts. 

![image](https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/1809bc49-fbbd-4402-8717-484f0403f9c1)

What are Mixture of Experts?
------
In general Mixture of Experts (MoE) is the method of using multiple expert models instead of using just a single big model for dividing a problem.

In Transformer blocks the MoE method is normally implemented by replacing the MLP after the Attention Layer with a Gate Network followed by the multiple expert MLPs.

In the RAPHAEL model a transformer block consists of four key components, the Self Attention Layer, the Cross Attention Layer, the Time-MoE Layer and the Space-MoE Layer as the following image shows:

<img width="414" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/e5d19f96-5af1-4ba7-97d7-80fc75212dfc">

What are Time-MoE?
------
Time-MoE is a MoE Layer which assigns the image in different denoising time steps to different expert models.

A Time-MoE layer constists of a Text Gate Network and the expert models as shown in the image below. It takes the time step and the features as input.

<img width="225" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/d3d0eb5b-1656-4932-83ca-f3c4638861f0">

The Time Gate Network does the assignment using the following formula: $trouter(t_i)=argmax(softmax(G′(E′_θ(t_i))+ϵ))$

The result of the assignments can be seen in the image below:

<img width="597" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/37ea0970-9829-4005-9946-cb757f6d62ba">

What are Space-MoE?
------
Space-MoE is a MoE Layer which assigns specific text tokens to image regions.

A Space-MoE layer constists of a Text Gate Network and the expert models as shown in the image below. It takes text as input.

<img width="335" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/3cefc66c-c482-49a0-a75b-f0bbafaba431">

The Text Gate Network does the assignment using the following formula: \$route(y_i)=argmax(softmax(G(E_θ(y_i))+ϵ))\$

The output of the Space-MoE Layer is built by taking the mean of all expert models, calculated by the following formula: 
\$\frac{1}{n_y} \Sigma_{i=1}^{n_y} e_{route(y_i)}(h′(x_t) \cdot M_i)\$

As a result of the Space-MoE Layer, as shown in the picture below, different categories activate different diffusion paths 

<img width="316" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/681216f0-19cd-4359-abef-51d55217c280">


What is Edge-supervised Learning
------
Edge-supervised Learing uses a edge detection module to extract boundary information, which is then used to supervise the model in preserving detailed image features.

The image below shows the attention map from the transformer block and the edges extracted by the edge detector next to the image.

<img width="698" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/74df4905-9526-4a0f-aaa6-a7faa9e8b7fa">

It shows that nearly twice as much people prefer the results of the model using Edge-supervised Learning than people prefering the model without it.

Experiments and Benchmarks
======

Experiments
------

<img width="634" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/86940ad0-1702-42b7-9c18-52113266eed3">

The hyperparameter \$\alpha\$ results in an optimal FID-5k score at about 0.2, smaller and larger alpha values decrease the performance.
The hyperparameter \$T_c\$ is optimal at about 500, for smaller values is slowly decreases, for bigger is decreases fast.

Experiments on the CLIP score show that model with Space-MoE, Time-MoE and Edge-supervised Learning is the best one and the best CLIP score is between 0.33 and 0.34.

The number of Experts can influence the FID-5k score and the computational complexity. The computational complexity steadily decreases with the increase of the number of 
experts. The FID-5k score gets better very fast at the beginning but flattens out quite fast, for the Time-MoE faster than for the Space-MoE.

Benchmarks
------

<img width="640" alt="image" src="https://github.com/Florian-de/floriandreyer.github.io/assets/64322175/38885b83-95db-40fd-aa1b-bfce8e309df2">

The table shows that RAPHAEL outperforms all competitors on the Zero-shot FID-30k. Especially in comparison with the two popular models Stable Diffusion and DALL-E it has a 21% and 37% better score.

Discussion
======

The most obvious advantage of the model is the accurate text in the generated images. Another advantage is the overall higher image quality, partly due to the Edge-supervised Learning. Another advantage is that MoE Architectures make the models more efficient due to sparse assignements. On the other hand is the high GPU usage for the training and the fact that the model is not open source.

References
======
<a id="1">[1]</a> 
Zeyue Xue et al. (May 2023). 
"RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths". 
[arXiv:2305.18295](https://arxiv.org/abs/2305.18295)

<a id="2">[2]</a> 
Jaskaran Bhatia (Jul 2023). 
[Summarizing the Evolution of Diffusion Models: Insight from Three Research Papers](https://medium.com/@jaskaranbhatia/summarizing-the-evolution-of-diffusion-models-insights-from-three-research-papers-6889339eba4). 

<a id="3">[3]</a> 
Kemal Erdem (Nov 2023). 
[Step by Step visual introduction to Diffusion Models](https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models). 

<a id="4">[4]</a> 
3Blue1Brown (Apr 2024). 
[Attention in transformers, visually explained](https://www.youtube.com/watch?v=eMlx5fFNoYc&t=71s). 


